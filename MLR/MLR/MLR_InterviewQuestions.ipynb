{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "Interview Questions:\n",
        "1.What is Normalization & Standardization and how is it helpful?\n",
        "-------------------------------------------------------------------\n",
        "Answer:\n",
        "Normalization and Standardization are feature scaling techniques. They rescale input variables so that differences in units and magnitude do not negatively affect model training or interpretation.\n",
        "Normalization and Standardization ensure that all predictors contribute fairly to the regression model, improve optimization efficiency, and enhance interpretability.\n",
        "\n",
        "Why Feature Scaling Is Needed in Multiple Linear Regression\n",
        "Consider a regression model with features like:\n",
        "Age → values around 20–60\n",
        "Annual Income → values around 50,000–2,000,000\n",
        "Number of years of experience → values around 1–30\n",
        "If these features are used as-is, variables with larger numeric ranges dominate the optimization process, especially when:\n",
        "Gradient Descent is used\n",
        "Regularization (Ridge/Lasso) is applied\n",
        "Multicollinearity exists,Normalization and Standardization fix this issue.\n",
        "Normalization (Min–Max Scaling):\n",
        "Normalization rescales features to a fixed range, usually [0, 1].\n",
        "Example:\n",
        "If Income ranges from 50,000 to 200,000:\n",
        "Income = 125,000 → Normalized value = 0.5\n",
        "Benefits in MLR:\n",
        "i)Keeps all features on the same scale\n",
        "ii)Helps gradient descent converge faster\n",
        "iii)Useful when features have known bounds\n",
        "Limitations:\n",
        "Sensitive to outliers\n",
        "Not ideal when data distribution is unknown or skewed\n",
        "Standardization (Z-score Scaling)\n",
        "Standardization rescales data to have:\n",
        "Mean = 0\n",
        "Standard Deviation = 1\n",
        "Example\n",
        "If:\n",
        "Mean income = 100,000\n",
        "Std deviation = 20,000\n",
        "Then:\n",
        "Income = 120,000 → Standardized value = 1\n",
        "Benefits in MLR:\n",
        "i)Handles outliers better than normalization\n",
        "ii)Makes coefficients comparable\n",
        "iii)Essential for Ridge and Lasso regression\n",
        "iv)Improves numerical stability\n",
        "\n",
        "2.What techniques can be used to address multicollinearity in multiple linear regression?\n",
        "=========================================================================================\n",
        "Answer:\n",
        "Multicollinearity does not harm prediction accuracy but severely affects coefficient stability and interpretability.\n",
        "Multicollinearity occurs in Multiple Linear Regression (MLR) when two or more independent variables are highly correlated, making coefficient estimates unstable and hard to interpret.\n",
        "Below are the most effective techniques to detect and address multicollinearity.\n",
        " Detect Multicollinearity (Before Fixing It)\n",
        "Correlation Matrix:\n",
        "High correlation (|r| > 0.8) between predictors is a warning sign.\n",
        "Variance Inflation Factor (VIF)\n",
        "VIF Value:Interpretation\n",
        "1\tNo multicollinearity\n",
        "1–5\tModerate\n",
        "> 5 (or 10)\tSevere\n",
        "Techniques to Address Multicollinearity\n",
        "1. Remove One of the Correlated Variables\n",
        "If two variables carry similar information, keep only one.\n",
        "Example:\n",
        "Height (cm) and Height (inches) → remove one\n",
        "Simple and effective\n",
        "May cause information loss\n",
        "2. Combine Correlated Features\n",
        "Create a single composite feature.\n",
        "Example:\n",
        "Math score + Science score → Average score\n",
        "Retains information\n",
        "Requires domain knowledge\n",
        "3. Feature Selection Techniques\n",
        "Stepwise Regression\n",
        "Forward selection\n",
        "Backward elimination\n",
        "Bidirectional elimination\n",
        "Removes redundant predictors automatically.\n",
        "4. Regularization Methods (Best Practical Solution)\n",
        "Ridge Regression (L2):\n",
        "Shrinks coefficients\n",
        "Keeps all features\n",
        "Lasso Regression (L1):\n",
        "Shrinks coefficients\n",
        "Can remove features completely\n",
        "Elastic Net;\n",
        "Combination of Ridge + Lasso\n",
        "Best when predictors are highly correlated\n",
        "Most recommended approach\n",
        "5. Principal Component Analysis (PCA)\n",
        "Transforms correlated variables into uncorrelated components.\n",
        "Removes multicollinearity entirely\n",
        "Coefficients lose direct interpretability\n",
        "6. Increase Sample Size\n",
        "More data can reduce instability caused by correlated predictors.\n",
        "Not always feasible\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNw7j6lCx/Y5aTQJBCO7WZS",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
